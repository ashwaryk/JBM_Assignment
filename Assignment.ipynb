{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization # Flatten will be used to applied between convolution layer and MLP(matrix to vector)\n",
    "from keras.layers import Convolution2D, MaxPooling2D , ZeroPadding2D # Convolution 2D for images, for text use convolution1D\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ashwa\\Desktop\\Image_Classification\\All_61326\\train/\"\n",
    "\n",
    "#Extracted list of all folders \n",
    "dirs = os.listdir(path)\n",
    "\n",
    "MODEL_NAME = 'Image_Convnet'\n",
    "\n",
    "## Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Total number of classes(images)\n",
    "nb_classes = 6\n",
    "nb_epoch = 50\n",
    "\n",
    "img_rows, img_cols = 150, 150     # input image dimensions\n",
    "img_channels = 3                  # images are RGB. as colored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function which resize images and stores in list(lst) and stores all train labels in list(target)\n",
    "def resize(lst,target,path,dirs):\n",
    "    print(\"Resizing the images!!\")\n",
    "    for folder in dirs:\n",
    "        for img in os.listdir(path+folder):\n",
    "            if os.path.isfile(path+folder+\"/\"+img):\n",
    "                im = Image.open(path+folder+\"/\"+img)\n",
    "                imResize = np.array(im.resize((150,150),Image.ANTIALIAS))\n",
    "                lst.append(imResize)\n",
    "                target.append(folder)\n",
    "    return(lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all elements of the list to array\n",
    "def lst_to_array(lst):\n",
    "    print(\"Converting to Numpy array!!\")\n",
    "    for i in range(0,len(lst)):\n",
    "        lst[i] = np.asarray(lst[i])\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encoder(target):\n",
    "    print(\"Label Encoder!!\")\n",
    "    # converts the character array to numeric array. Assigns levels to unique labels.\n",
    "    le = LabelEncoder()                  \n",
    "    le.fit(target)\n",
    "    train_labels = le.transform(target)\n",
    "    #val_labels = le.transform(val_target)# convert string into numbers\n",
    "    \n",
    "    print(le.classes_)\n",
    "    \n",
    "    # find unique train labels\n",
    "    print(np.unique(target, return_counts=True))    \n",
    "    print(len(train_labels))\n",
    "    \n",
    "    # Converted train and validation labels to one hot encoding\n",
    "    labels_train = to_categorical(np.asarray(train_labels))\n",
    "\n",
    "    return labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(lst, labels_train):\n",
    "    print(\"Fit the Model!!\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(6, activation = \"softmax\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    ## Compile the model\n",
    "    #Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    adam = Adam(lr = 0.001,decay= 0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', # Becoz it is multi class classification problem\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ## Fit the model\n",
    "    model.fit(np.array(lst),labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_datagen(model):\n",
    "    ## Reading the Test Images from the repective directories\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        'test_61326/',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    ## Predictions for the test files\n",
    "    test_prob = model.predict_generator(test_generator) # this returns the probabilities\n",
    "    test_pred_classes = np.argmax(test_prob, axis=1) # convert probabilities to classes\n",
    "    print(test_pred_classes)\n",
    "    print(test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # list to store all images of train data \n",
    "    lst = []\n",
    "    # list to store all labels of train data\n",
    "    target = []\n",
    "\n",
    "    ## calling function to extract train image data and their label in list\n",
    "    lst, target = resize(lst, target, path, dirs)\n",
    "\n",
    "    ## Converting the list of images to an array\n",
    "    lst = lst_to_array(lst)\n",
    "    \n",
    "    ## Encoding the labels\n",
    "    labels_train = label_encoder(target)\n",
    "    \n",
    "    ## Build a CNN Model\n",
    "    cnn_model(lst, labels_train)\n",
    "    \n",
    "    ## Predict on test data\n",
    "    test_datagen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating checkpoints and .pb file\n",
    "\n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
