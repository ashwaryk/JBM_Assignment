{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization # Flatten will be used to applied between convolution layer and MLP(matrix to vector)\n",
    "from keras.layers import Convolution2D, MaxPooling2D , ZeroPadding2D # Convolution 2D for images, for text use convolution1D\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ashwa\\Desktop\\Image_Classification\\All_61326\\train/\"\n",
    "\n",
    "#Extracted list of all folders \n",
    "dirs = os.listdir(path)\n",
    "\n",
    "MODEL_NAME = 'Image_Convnet'\n",
    "\n",
    "## Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Total number of classes(images)\n",
    "nb_classes = 6\n",
    "nb_epoch = 50\n",
    "\n",
    "img_rows, img_cols = 150, 150     # input image dimensions\n",
    "img_channels = 3                  # images are RGB. as colored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function which resize images and stores in list(lst) and stores all train labels in list(target)\n",
    "def resize(lst,target,path,dirs):\n",
    "    print(\"Resizing the images!!\")\n",
    "    for folder in dirs:\n",
    "        for img in os.listdir(path+folder):\n",
    "            if os.path.isfile(path+folder+\"/\"+img):\n",
    "                im = Image.open(path+folder+\"/\"+img)\n",
    "                imResize = np.array(im.resize((150,150),Image.ANTIALIAS))\n",
    "                lst.append(imResize)\n",
    "                target.append(folder)\n",
    "    return(lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all elements of the list to array\n",
    "def lst_to_array(lst):\n",
    "    print(\"Converting to Numpy array!!\")\n",
    "    for i in range(0,len(lst)):\n",
    "        lst[i] = np.asarray(lst[i])\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(target):\n",
    "    print(\"Label Encoder!!\")\n",
    "    # converts the character array to numeric array. Assigns levels to unique labels.\n",
    "    le = LabelEncoder()                  \n",
    "    le.fit(target)\n",
    "    train_labels = le.transform(target)\n",
    "    #val_labels = le.transform(val_target)# convert string into numbers\n",
    "    \n",
    "    print(le.classes_)\n",
    "    \n",
    "    # find unique train labels\n",
    "    print(np.unique(target, return_counts=True))    \n",
    "    print(len(train_labels))\n",
    "    \n",
    "    # Converted train and validation labels to one hot encoding\n",
    "    labels_train = to_categorical(np.asarray(train_labels))\n",
    "\n",
    "    return labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(lst, labels_train):\n",
    "    print(\"Fit the Model!!\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(6, activation = \"softmax\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    ## Compile the model\n",
    "    #Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    adam = Adam(lr = 0.001,decay= 0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', # Becoz it is multi class classification problem\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ## Fit the model\n",
    "    model.fit(np.array(lst),labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # list to store all images of train data \n",
    "    lst = []\n",
    "    # list to store all labels of train data\n",
    "    target = []\n",
    "\n",
    "    ## calling function to extract train image data and their label in list\n",
    "    lst, target = resize(lst, target, path, dirs)\n",
    "\n",
    "    ## Converting the list of images to an array\n",
    "    lst = lst_to_array(lst)\n",
    "    \n",
    "    ## Encoding the labels\n",
    "    labels_train = label_encoder(target)\n",
    "    \n",
    "    ## Build a CNN Model\n",
    "    cnn_model(lst, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing the images!!\n",
      "Converting to Numpy array!!\n",
      "Label Encoder!!\n",
      "['61326 Scratch Mark' '61326 Slot Damage' '61326 Thinning' '61326 Wrinkle'\n",
      " '61326_0k_back' '61326_ok_front']\n",
      "(array(['61326 Scratch Mark', '61326 Slot Damage', '61326 Thinning',\n",
      "       '61326 Wrinkle', '61326_0k_back', '61326_ok_front'],\n",
      "      dtype='<U18'), array([385, 330, 488, 390, 415, 406], dtype=int64))\n",
      "2414\n",
      "Fit the Model!!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 36992)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2367552   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 2,465,862\n",
      "Trainable params: 2,465,606\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 2172 samples, validate on 242 samples\n",
      "Epoch 1/20\n",
      "2172/2172 [==============================] - 20s 9ms/step - loss: 2.1307 - acc: 0.3541 - val_loss: 1.6853 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "2172/2172 [==============================] - 8s 3ms/step - loss: 1.4076 - acc: 0.4894 - val_loss: 3.3635 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "2172/2172 [==============================] - 8s 4ms/step - loss: 1.0654 - acc: 0.5737 - val_loss: 1.5956 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.8739 - acc: 0.6220 - val_loss: 1.7964 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.6306 - acc: 0.7587 - val_loss: 1.5789 - val_acc: 0.1033\n",
      "Epoch 6/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.4435 - acc: 0.8398 - val_loss: 1.3382 - val_acc: 0.3843\n",
      "Epoch 7/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.3341 - acc: 0.8927 - val_loss: 1.2567 - val_acc: 0.4008\n",
      "Epoch 8/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.2772 - acc: 0.9190 - val_loss: 0.8702 - val_acc: 0.6198\n",
      "Epoch 9/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.2400 - acc: 0.9300 - val_loss: 1.1165 - val_acc: 0.4380\n",
      "Epoch 10/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.2113 - acc: 0.9411 - val_loss: 0.6424 - val_acc: 0.7107\n",
      "Epoch 11/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.1721 - acc: 0.9535 - val_loss: 0.2344 - val_acc: 0.9711\n",
      "Epoch 12/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.1546 - acc: 0.9595 - val_loss: 0.2623 - val_acc: 0.9298\n",
      "Epoch 13/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.1255 - acc: 0.9696 - val_loss: 0.2521 - val_acc: 0.9132\n",
      "Epoch 14/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.1120 - acc: 0.9774 - val_loss: 0.2206 - val_acc: 0.9463\n",
      "Epoch 15/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0970 - acc: 0.9811 - val_loss: 0.1882 - val_acc: 0.9174\n",
      "Epoch 16/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0943 - acc: 0.9807 - val_loss: 0.1766 - val_acc: 0.9339\n",
      "Epoch 17/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0869 - acc: 0.9839 - val_loss: 0.1157 - val_acc: 0.9628\n",
      "Epoch 18/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0751 - acc: 0.9866 - val_loss: 0.0650 - val_acc: 0.9876\n",
      "Epoch 19/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0819 - acc: 0.9793 - val_loss: 0.0683 - val_acc: 0.9793\n",
      "Epoch 20/20\n",
      "2172/2172 [==============================] - 7s 3ms/step - loss: 0.0639 - acc: 0.9885 - val_loss: 0.0349 - val_acc: 0.9917\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Reading the Test Images from the repective directories\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'test_61326/',  # this is the target directory\n",
    "    target_size=(150, 150),  # all images will be resized to 150x150\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a89d008e7cb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Predictions for the test files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this returns the probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_pred_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# convert probabilities to classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## Predictions for the test files\n",
    "test_prob = model.predict_generator(test_generator) # this returns the probabilities\n",
    "test_pred_classes = np.argmax(test_prob, axis=1) # convert probabilities to classes\n",
    "print(test_pred_classes)\n",
    "print(test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating checkpoints and .pb file\n",
    "\n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
